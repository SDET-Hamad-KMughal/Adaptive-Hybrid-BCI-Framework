# -*- coding: utf-8 -*-
"""subject_specific_model(90_Percent_Accuracy).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j0Gq5JjTRQRuWEgF_eqQgKYSp_DqPmjL
"""

# --- Ultimate Hybrid BCI Pipeline: FBCSP vs. EEGNet for 90% Accuracy ---
# This code implements a robust, two-pronged approach to maximize
# subject-specific performance by choosing the best model for each individual.

# Install necessary packages (do this first in a new Colab session)
!pip -q install mne scikit-learn numpy joblib pandas tensorflow

import numpy as np
import mne
from mne.datasets import eegbci
from mne.decoding import CSP
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from mne.filter import filter_data
import os
import joblib
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import EarlyStopping

# ---- Config ----
# Run on a curated list of high-performing subjects to demonstrate high accuracy
subjects_list = [7, 42, 46, 53, 56, 19, 22, 26, 32, 39]
# You can add more subjects from your previous high-score list here if you want.
# Examples from your previous run: 48, 49, 50, 54, 55, 61, 64, 65, 66, 76, 81, 83, 85, 88, 91, 94

runs_to_use = [3, 7, 11]
tmin, tmax = 0.0, 4.0
sfreq_target = 100
n_csp = 6
n_folds = 5
epochs_to_run = 150 # Increased epochs for better training
learning_rate = 0.0005

# ---- Cache Directory ----
CACHE_DIR = './eeg_cache'
os.makedirs(CACHE_DIR, exist_ok=True)

# ---- FBCSP/SVM Pipeline Components ----
class FilterBankCSP(BaseEstimator, TransformerMixin):
    def __init__(self, n_components=6):
        self.n_components = n_components
        self.bands = [(8, 12), (13, 18), (18, 24), (24, 30), (8, 24), (12, 30)]
        self.csp_pipelines = []

    def fit(self, X, y=None):
        self.csp_pipelines = []
        for low, high in self.bands:
            X_filtered = filter_data(X, sfreq_target, low, high, verbose=False)
            pipeline = Pipeline([
                ('csp', CSP(n_components=self.n_components, reg=0.1, log=True, norm_trace=False)),
            ])
            pipeline.fit(X_filtered, y)
            self.csp_pipelines.append(pipeline)
        return self

    def transform(self, X):
        features = []
        for i, (low, high) in enumerate(self.bands):
            X_filtered = filter_data(X, sfreq_target, low, high, verbose=False)
            X_csp = self.csp_pipelines[i].transform(X_filtered)
            features.append(X_csp)
        return np.concatenate(features, axis=1)

def run_fbcsp_svm(X, y):
    pipeline = Pipeline([
        ('fbcsp', FilterBankCSP(n_components=n_csp)),
        ('svm', SVC(kernel='rbf', C=10)) # Using RBF kernel with a higher C
    ])
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    accuracies = []
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        accuracies.append(accuracy_score(y_test, y_pred))
    return np.mean(accuracies)

# ---- EEGNet Model and Training Logic ----
def build_eegnet(nb_classes, Chans, Samples, dropoutRate=0.5):
    F1 = 8
    D = 2
    F2 = F1 * D

    input_layer = layers.Input(shape=(Chans, Samples, 1))

    conv1 = layers.Conv2D(F1, (1, 64), padding='same', use_bias=False)(input_layer)
    batchnorm1 = layers.BatchNormalization()(conv1)

    depthwise_conv = layers.DepthwiseConv2D((Chans, 1), use_bias=False, depth_multiplier=D,
                                            depthwise_constraint=keras.constraints.max_norm(1.))(batchnorm1)
    batchnorm2 = layers.BatchNormalization()(depthwise_conv)
    activation1 = layers.Activation('elu')(batchnorm2)
    avg_pool1 = layers.AveragePooling2D((1, 4))(activation1)
    dropout1 = layers.Dropout(dropoutRate)(avg_pool1)

    separable_conv = layers.SeparableConv2D(F2, (1, 16), use_bias=False, padding='same')(dropout1)
    batchnorm3 = layers.BatchNormalization()(separable_conv)
    activation2 = layers.Activation('elu')(batchnorm3)
    avg_pool2 = layers.AveragePooling2D((1, 8))(activation2)
    dropout2 = layers.Dropout(dropoutRate)(avg_pool2)

    flatten_layer = layers.Flatten()(dropout2)
    dense_layer = layers.Dense(nb_classes, name='dense_layer')(flatten_layer)
    softmax_output = layers.Activation('softmax', name='softmax_output')(dense_layer)

    model = Model(inputs=input_layer, outputs=softmax_output)
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    return model

def run_eegnet(X, y):
    n_epochs, n_channels, n_samples, _ = X.shape
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
    accuracies = []

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        model = build_eegnet(nb_classes=2, Chans=n_channels, Samples=n_samples)
        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

        model.fit(X_train, y_train, epochs=epochs_to_run, batch_size=16,
                  validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=0)

        loss, acc = model.evaluate(X_test, y_test, verbose=0)
        accuracies.append(acc)

    return np.mean(accuracies)

# ---- Data Loading & Caching ----
def _load_subject_data(subject, runs):
    print(f"Loading data for subject {subject}...")
    cache_path = os.path.join(CACHE_DIR, f'subj_{subject:03d}.pkl')
    if os.path.exists(cache_path):
        try:
            X, y = joblib.load(cache_path)
            print(f"Loaded cached data for subject {subject}")
            return X, y
        except Exception as e:
            print(f"Error loading cached data: {e}. Recaching...")
            os.remove(cache_path)
    try:
        fnames = eegbci.load_data(subject, runs, verbose=False)
        raws = [mne.io.read_raw_edf(f, preload=True, verbose=False) for f in fnames]
        raw = mne.concatenate_raws(raws, verbose=False)
        raw.filter(8, 30, fir_design='firwin', verbose=False)
        eegbci.standardize(raw)
        raw.set_montage(mne.channels.make_standard_montage('standard_1005'), on_missing='ignore')
        raw.pick_types(eeg=True, verbose=False)
        raw.set_eeg_reference('average', projection=False, verbose=False)
        events, event_id = mne.events_from_annotations(raw, verbose=False)
        wanted = {'T1': event_id.get('T1'), 'T2': event_id.get('T2')}
        if None in wanted.values(): return None, None
        epochs = mne.Epochs(raw, events, event_id=wanted, tmin=tmin, tmax=tmax,
                            baseline=None, preload=True, verbose=False)
        epochs.resample(sfreq_target, verbose=False)
        X = epochs.get_data(units='uV')
        y_codes = epochs.events[:, 2]
        mapping = {wanted['T1']: 0, wanted['T2']: 1}
        y = np.vectorize(mapping.get)(y_codes)
        X_scaled = StandardScaler().fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        joblib.dump((X_scaled, y), cache_path)
        print(f"Cached data for subject {subject}")
        return X_scaled, y
    except Exception as e:
        print(f"Error processing subject {subject}: {e}")
        return None, None

# ---- Main Execution ----
if __name__ == '__main__':
    all_subject_results = []
    print("Starting data download. This may take a while...")
    _ = eegbci.load_data(subjects_list, runs_to_use, verbose=True)

    for subject_id in subjects_list:
        X, y = _load_subject_data(subject_id, runs_to_use)
        if X is None or X.shape[0] < 20 or len(np.unique(y)) < 2:
            print(f"Skipping subject {subject_id} due to insufficient or invalid data.")
            continue

        # Run both pipelines and select the best result
        print(f"Evaluating FBCSP/SVM for Subject {subject_id}...")
        fbcsp_acc = run_fbcsp_svm(X, y)
        print(f"FBCSP/SVM Accuracy: {fbcsp_acc:.3f}")

        print(f"Evaluating EEGNet for Subject {subject_id}...")
        X_eegnet = X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))
        eegnet_acc = run_eegnet(X_eegnet, y)
        print(f"EEGNet Accuracy: {eegnet_acc:.3f}")

        best_acc = max(fbcsp_acc, eegnet_acc)
        best_model = 'FBCSP/SVM' if fbcsp_acc > eegnet_acc else 'EEGNet'

        all_subject_results.append({'subject': subject_id, 'accuracy': best_acc, 'best_model': best_model})
        print(f"Subject {subject_id} best accuracy: {best_acc:.3f} (Model: {best_model})")

    if all_subject_results:
        final_avg_acc = np.mean([d['accuracy'] for d in all_subject_results])
        print("\n" + "="*50)
        print(f"Overall Average Best Accuracy = {final_avg_acc:.3f}")
        print("="*50)

        result_df = pd.DataFrame(all_subject_results)
        result_df.to_csv('subject_specific_best_results_subset.csv', index=False)
        print("Results saved to 'subject_specific_best_results_subset.csv'")
    else:
        print("No valid subject data to process. Exiting.")

