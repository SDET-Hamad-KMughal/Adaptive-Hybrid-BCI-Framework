# -*- coding: utf-8 -*-
"""loso_cross_validation_baseline(52_Percent_Accuracy).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UWgsG4Jg4qef-wtQG7bIqlVzTvaQ6FQa
"""

# ---- Full Refactored and Optimized Code ----
# Install necessary packages (only if you haven't already)
!pip -q install mne scikit-learn numpy joblib pandas

import numpy as np
import mne
from mne.datasets import eegbci
from mne.decoding import CSP
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.metrics import accuracy_score
import os
import joblib
from joblib import Parallel, delayed
import pandas as pd

# ---- Config ----
MAX_SUBJECTS_TO_USE = 109
subjects_list = list(range(1, 110))[:MAX_SUBJECTS_TO_USE]
runs_to_use = [3, 7, 11]
tmin, tmax = 0.0, 4.0
sfreq_target = 128
n_csp = 6
N_JOBS = -1  # Use all available CPU cores

# ---- Cache Directory ----
CACHE_DIR = './eeg_cache'
os.makedirs(CACHE_DIR, exist_ok=True)

# ---- Data Loading & Caching Logic ----
def _load_subject_data(subject, runs):
    fnames = eegbci.load_data(subject, runs, verbose=False)
    raws = [mne.io.read_raw_edf(f, preload=True, verbose=False) for f in fnames]
    raw = mne.concatenate_raws(raws, verbose=False)
    eegbci.standardize(raw)
    montage = mne.channels.make_standard_montage('standard_1005')
    raw.set_montage(montage, on_missing='ignore', verbose=False)
    raw.pick_types(eeg=True, verbose=False)
    raw.set_eeg_reference('average', projection=False, verbose=False)

    events, event_id = mne.events_from_annotations(raw, verbose=False)
    wanted = {'T1': event_id.get('T1'), 'T2': event_id.get('T2')}
    if None in wanted.values():
        return None, None

    epochs = mne.Epochs(raw, events, event_id=wanted, tmin=tmin, tmax=tmax,
                        baseline=None, preload=True, verbose=False)
    epochs.resample(sfreq_target, verbose=False)
    X = epochs.get_data()
    y_codes = epochs.events[:, 2]
    mapping = {wanted['T1']: 0, wanted['T2']: 1}
    y = np.vectorize(mapping.get)(y_codes)
    return X, y

def cache_and_load_all_subjects():
    print("Caching all subjects (if not already cached)...")
    all_data = []

    # Download all files upfront
    print("Starting data download for all subjects. This may take a while...")
    _ = eegbci.load_data(subjects_list, runs_to_use, verbose=True)

    for subject in subjects_list:
        cache_path = os.path.join(CACHE_DIR, f'subj_{subject:03d}.pkl')
        if os.path.exists(cache_path):
            try:
                X, y = joblib.load(cache_path)
                print(f"Loaded cached data for subject {subject}")
            except Exception as e:
                print(f"Error loading cached data for subject {subject}: {e}. Recaching...")
                X, y = _load_subject_data(subject, runs_to_use)
                if X is not None:
                    joblib.dump((X, y), cache_path)
                    print(f"Recached data for subject {subject}")
                else:
                    print(f"Skipping subject {subject} due to missing data.")
        else:
            try:
                X, y = _load_subject_data(subject, runs_to_use)
                if X is not None:
                    joblib.dump((X, y), cache_path)
                    print(f"Cached data for subject {subject}")
                else:
                    print(f"Skipping subject {subject} due to missing data.")
            except Exception as e:
                print(f"Error processing subject {subject}: {e}")
                X, y = None, None

        if X is not None:
            all_data.append({'subject': subject, 'X': X, 'y': y})

    return all_data

# ---- LOSO Fold Function for Parallel Execution (Optimized for Memory) ----
def run_loso_fold(test_subj_idx, all_data_array):
    test_subject = all_data_array[test_subj_idx]['subject']
    print(f"Running LOSO fold for test subject {test_subject}")

    X_test, y_test = all_data_array[test_subj_idx]['X'], all_data_array[test_subj_idx]['y']

    Xtr_list, ytr_list = [], []
    for i in range(len(all_data_array)):
        if i == test_subj_idx:
            continue
        Xtr_list.append(all_data_array[i]['X'])
        ytr_list.append(all_data_array[i]['y'])

    if not Xtr_list:
        return None

    X_train = np.concatenate(Xtr_list, axis=0)
    y_train = np.concatenate(ytr_list, axis=0)

    # CSP Feature Extraction
    csp = CSP(n_components=n_csp, reg=None, log=True, norm_trace=False)
    Xtr_f = csp.fit_transform(X_train, y_train)
    Xte_f = csp.transform(X_test)

    # LDA Classification
    clf = LDA()
    clf.fit(Xtr_f, y_train)
    y_pred = clf.predict(Xte_f)

    acc = accuracy_score(y_test, y_pred)
    return acc

# ---- Main Execution ----
if __name__ == '__main__':
    all_subject_data = cache_and_load_all_subjects()

    if not all_subject_data:
        print("No valid subject data to process. Exiting.")
    else:
        print("Starting parallel LOSO cross-validation...")
        # CRUCIAL FIX: Use backend="threading" to share memory and avoid OOM errors
        all_accuracies = Parallel(n_jobs=N_JOBS, backend="threading")(
            delayed(run_loso_fold)(idx, all_subject_data)
            for idx in range(len(all_subject_data))
        )

        all_accuracies = [a for a in all_accuracies if a is not None]
        final_avg_acc = np.mean(all_accuracies)

        print("\n" + "="*50)
        print(f"Final Average Accuracy = {final_avg_acc:.3f}")
        print("="*50)

        # Save Results
        valid_subjects = [d['subject'] for d in all_subject_data]
        result_df = pd.DataFrame({
            'Subject ID': valid_subjects,
            'Accuracy': all_accuracies
        })
        result_df.to_csv('loso_results.csv', index=False)
        print("Results saved to 'loso_results.csv'")

